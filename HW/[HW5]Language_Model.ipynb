{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[HW5]Language_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR26RFkwXtvi"
      },
      "source": [
        "# **[HW5] Language Model**\n",
        "1. DataLoader\n",
        "2. Model\n",
        "3. Trainer\n",
        "4. Generation\n",
        "\n",
        "이번 실습에서는 RNN기반의 Language Model를 구현해서 텍스트를 직접 생성해보는 실습을 진행해보겠습니다.\n",
        "\n",
        "- dataset: WikiText2 (https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2)\n",
        "- model: LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crVJ36mMlaXP"
      },
      "source": [
        "\n",
        "\n",
        "## Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpvlE_XOWS33"
      },
      "source": [
        "런타임의 유형을 변경해줍니다.\n",
        "\n",
        "상단 메뉴에서 [런타임]->[런타임유형변경]->[하드웨어가속기]->[GPU]\n",
        "\n",
        "변경 이후 아래의 cell을 실행 시켰을 때, torch.cuda.is_avialable()이 True가 나와야 합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqVdEuPQzMAH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o3-HPdHLZma"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import tqdm\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1GnKJCB4T_Q"
      },
      "source": [
        "# 1. DataLoader\n",
        "\n",
        "이전의 실습들에서 사용한것과 마찬가지로, PyTorch style의 dataloader를 먼저 만들어 두겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcNl0aWbS0OA"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "저희가 이번 실습에서 사용할 데이터셋은 Wikipedia에 있는 영문 글들을 가져온 WikiTree dataset입니다.\n",
        "저희가 불러올 데이터는 가장 작은 WikiTree dataset에서 자주 사용되지 않는 단어나 영어가 아닌 단어들은 <unk>으로 이미 전처리가 되어있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKf8zNuISiC2"
      },
      "source": [
        "import urllib\n",
        "with urllib.request.urlopen('https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/02-intermediate/language_model/data/train.txt') as f:\n",
        "    data = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBLNOlRKSpOI"
      },
      "source": [
        "print('num_sentence:',len(data))\n",
        "data[100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfLTv1EPbSwj"
      },
      "source": [
        "seq_length_list = []\n",
        "for line in data:\n",
        "    seq_length_list.append(len(line.split()))\n",
        "\n",
        "counts, bins = np.histogram(seq_length_list, bins=20)\n",
        "plt.hist(bins[:-1], bins, weights=counts)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SdattmOcRwC"
      },
      "source": [
        "데이터에 있는 문장 길이들의 histogram을 볼 때 대부분의 data의 문장 길이가 50에 미치지 못하기 때문에 \\\\\n",
        "model에 집어넣을 최대 문장 길이를 50으로 세팅해두도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7MuFqsKcd4U"
      },
      "source": [
        "max_seq_len = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyMpsyX8TwYy"
      },
      "source": [
        "### Build Dictionary\n",
        "\n",
        "먼저 text 데이터를 모델에 넣어주기 위해서는 text에 존재하는 단어들을 index로 변환해주어야 합니다.\n",
        "\n",
        "이를 위해서는 단어를 index로 변환해주는 word2idx dictionary와 다시 index를 단어로 변환해주는 idx2word dictionary를 만들어야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZmyZhcpTvZz"
      },
      "source": [
        "def build_dictionary(data, max_seq_len):\n",
        "    word2idx = {}\n",
        "    idx2word = {}\n",
        "    ## Build Dictionary\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "    idx2word[0] = '<pad>'\n",
        "    idx2word[1] = '<unk>'\n",
        "    idx = 2\n",
        "    for line in data:\n",
        "        words = line.decode('utf-8').split()\n",
        "        words = words[:max_seq_len]        \n",
        "        ### Build Dictionary to convert word to index and index to word\n",
        "        ### YOUR CODE HERE (~ 5 lines)\n",
        "\n",
        "    return word2idx, idx2word\n",
        "\n",
        "word2idx, idx2word = build_dictionary(data, max_seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPfV0OTc4Xdr"
      },
      "source": [
        "if len(word2idx) == len(idx2word) == 10000:\n",
        "    print(\"Test Passed!\")\n",
        "else:\n",
        "    raise AssertionError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me_m8njoXHrv"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "이제 앞서 만든 dictionary를 이용해서 text로된 데이터셋을 index들로 변환시키겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6fuARgzXEDU"
      },
      "source": [
        "def preprocess(data, word2idx, idx2word, max_seq_len):\n",
        "    tokens = []\n",
        "    for line in data:\n",
        "        words = line.decode('utf-8').split()\n",
        "        words = words[:max_seq_len]\n",
        "        ### Convert dataset with tokens\n",
        "        ### For each line, append <pad> token to match the number of max_seq_len\n",
        "        ### YOUR CODE HERE (~ 4 lines)\n",
        "      \n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess(data, word2idx, idx2word, max_seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjyvqMgbZnfP"
      },
      "source": [
        "if len(tokens) == 2103400:\n",
        "    print(\"Test Passed!\")\n",
        "else:\n",
        "    raise AssertionError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmQxX3BH-SAv"
      },
      "source": [
        "이제 전처리된 Token들을 문장 단위의 배열로 변환시켜 두겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knMvtp23-Jye"
      },
      "source": [
        "tokens = np.array(tokens).reshape(-1, max_seq_len)\n",
        "print(tokens.shape)\n",
        "tokens[100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pceBqmtTZ9g9"
      },
      "source": [
        "### DataLoader\n",
        "\n",
        "이제 전처리된 dataset을 활용하여 PyTorch style의 dataset과 dataloader를 만들도록 하겠습니다.\n",
        "\n",
        "Token형태의 데이터를 PyTorch 스타일의 dataset으로 만들 때 주의할 점은, 추후 embedding matrix에서 indexing을 해주기 위해서 각 token이 LongTensor 형태로 정의되어야 한다는 점입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hAwhG1K9iBI"
      },
      "source": [
        "class LMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokens):\n",
        "        super(LMDataset, self).__init__()\n",
        "        self.PAD = 0\n",
        "        self.UNK = 1\n",
        "        self.tokens = tokens\n",
        "        self._getitem(2)\n",
        "\n",
        "    def _getitem(self, index):\n",
        "        X = self.tokens[index]\n",
        "        y = np.concatenate((X[1:], [self.PAD]))\n",
        "\n",
        "        X = torch.from_numpy(X).unsqueeze(0).long()\n",
        "        y = torch.from_numpy(y).unsqueeze(0).long()\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.tokens[index]\n",
        "        y = np.concatenate((X[1:], [self.PAD]))\n",
        "\n",
        "        X = torch.from_numpy(X).long()\n",
        "        y = torch.from_numpy(y).long()\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiLNqM6kAda1"
      },
      "source": [
        "batch_size = 64\n",
        "dataset = LMDataset(tokens)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(len(dataset))\n",
        "print(len(dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1nhBnqWxw4a"
      },
      "source": [
        "# 2. Model\n",
        "\n",
        "이번 section에서는 Language Modeling을 위한 Recurrent Model을 직접 만들어보도록 하겠습니다.\n",
        "\n",
        "Standard한 Recurrent Neural Network (RNN) model은 vanishing gradient 문제에 취약하기 때문에, 이번 실습에서는 변형된 RNN구조인 LSTM model을 활용하도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOoNVt3MDOjl"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lycT_9vwaJN"
      },
      "source": [
        "LSTM model의 전체적인 구조와 각 gate의 수식은 아래와 같습니다.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1n93tpNW55Xl4GxZNcJcbUVRhuNCGH38h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1h6nfvYwN8n"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1nH9U5iD9cO6OVVTbrx-LjypRvcWzbOCU)\n",
        "\n",
        "LSTM의 자세한 동작방식이 궁금하신 분은 아래의 블로그를 참조해주세요.\n",
        "\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDNAysVqxxOk"
      },
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        # input-gate\n",
        "        self.Wi = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # forget-gate\n",
        "        self.Wf = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # gate-gate\n",
        "        self.Wg = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # output-gate\n",
        "        self.Wo = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "        # non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, h_0, c_0):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            input (x): [batch_size, input_size]\n",
        "            hidden_state (h_0): [batch_size, hidden_size]\n",
        "            cell_state (c_0): [batch_size, hidden_size]\n",
        "        Outputs\n",
        "            next_hidden_state (h_1): [batch_size, hidden_size]\n",
        "            next_cell_state (c_1): [batch_size, hidden_size]    \n",
        "        \"\"\"\n",
        "        h_1, c_1 = None, None\n",
        "        input = torch.cat((x, h_0), 1) \n",
        "        # Implement LSTM cell as noted above\n",
        "        ### YOUR CODE HERE (~ 6 lines)\n",
        "\n",
        "        return h_1,c_1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Tff2VCJ56D"
      },
      "source": [
        "def test_lstm():\n",
        "    batch_size = 2\n",
        "    input_size = 5\n",
        "    hidden_size = 3\n",
        "\n",
        "    #torch.manual_seed(1234)\n",
        "    lstm = LSTMCell(input_size ,hidden_size)\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.constant_(m.weight, 0.1)\n",
        "            m.bias.data.fill_(0.01)\n",
        "    lstm.apply(init_weights)\n",
        "\n",
        "    x = torch.ones(batch_size, input_size) \n",
        "    hx = torch.zeros(batch_size, hidden_size) \n",
        "    cx = torch.zeros(batch_size, hidden_size)\n",
        "\n",
        "    hx, cx = lstm(x, hx, cx)\n",
        "    assert hx.detach().allclose(torch.tensor([[0.1784, 0.1784, 0.1784], \n",
        "                                              [0.1784, 0.1784, 0.1784]]), atol=2e-1), \\\n",
        "            f\"Output of the hidden state does not match.\"\n",
        "    assert cx.detach().allclose(torch.tensor([[0.2936, 0.2936, 0.2936], \n",
        "                                              [0.2936, 0.2936, 0.2936]]), atol=2e-1), \\\n",
        "            f\"Output of the cell state does not match.\"\n",
        "\n",
        "    print(\"==LSTM cell test passed!==\")\n",
        "\n",
        "test_lstm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DxU-78B33dG"
      },
      "source": [
        "## Language Model\n",
        "\n",
        "이제, 위에서 정의한 LSTM Cell을 활용해서 아래와 같은 Langauge Model을 만들어보도록 하겠습니다.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1nMAbL-g31nERM44dgohA3k9Vj_92hIh-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0U2s0hux_n6"
      },
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, input_size=64, hidden_size=64, vocab_size=10000):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        \n",
        "        self.input_layer = nn.Embedding(vocab_size, input_size)\n",
        "        self.hidden_layer = LSTMCell(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, hx, cx, predict=False):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            input (x): [batch_size]\n",
        "            hidden_state (h_0): [batch_size, hidden_size]\n",
        "            cell_state (c_0): [batch_size, hidden_size]\n",
        "            predict: whether to predict and sample the next word\n",
        "        Outputs\n",
        "            output (ox): [batch_size, hidden_size]\n",
        "            next_hidden_state (h_1): [batch_size, hidden_size]\n",
        "            next_cell_state (c_1): [batch_size, hidden_size]    \n",
        "        \"\"\"\n",
        "        x = self.input_layer(x)\n",
        "        hx, cx = self.hidden_layer(x, hx, cx)\n",
        "        ox = self.output_layer(hx)\n",
        "\n",
        "        if predict == True:\n",
        "            probs = F.softmax(ox, dim=1)\n",
        "            # torch distribution allows sampling operation\n",
        "            # see https://pytorch.org/docs/stable/distributions.html\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            ox = dist.sample()\n",
        "\n",
        "        return ox, hx, cx  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-ZpuMhsbBS8"
      },
      "source": [
        "# 3. Trainer\n",
        "\n",
        "자 이제 위에서 구현한 dataloader와 langauge model을 활용해서 모델의 학습을 진행해보도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7TY7HmvbRlB"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, \n",
        "                 word2idx, \n",
        "                 idx2word,\n",
        "                 dataloader, \n",
        "                 model, \n",
        "                 criterion,\n",
        "                 optimizer, \n",
        "                 device):\n",
        "        \"\"\"\n",
        "        dataloader: dataloader\n",
        "        model: langauge model\n",
        "        criterion: loss function to evaluate the model (e.g., BCE Loss)\n",
        "        optimizer: optimizer for model\n",
        "        \"\"\"\n",
        "        self.word2idx = word2idx\n",
        "        self.idx2word = idx2word\n",
        "        self.dataloader = dataloader\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        \n",
        "    def train(self, epochs = 1):\n",
        "        self.model.to(self.device)\n",
        "        start_time = time.time()\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "            for iter, (x_batch, y_batch) in tqdm.tqdm(enumerate(self.dataloader)):\n",
        "                self.model.train()\n",
        "                \n",
        "                batch_size, max_seq_len = x_batch.shape\n",
        "                x_batch = x_batch.to(self.device)\n",
        "                y_batch = y_batch.to(self.device)\n",
        "\n",
        "                # initial hidden-states\n",
        "                hx = torch.zeros(batch_size, hidden_size).to(self.device)\n",
        "                cx = torch.zeros(batch_size, hidden_size).to(self.device)\n",
        "\n",
        "                # Implement LSTM operation\n",
        "                ox_batch = []\n",
        "                # Get output logits for each time sequence and append to the list, ox_batch\n",
        "                # YOUR CODE HERE (~ 4 lines)\n",
        "\n",
        "\n",
        "\n",
        "                # outputs are ordered by the time sequence\n",
        "                ox_batch = torch.cat(ox_batch).reshape(max_seq_len, batch_size, -1)\n",
        "                ox_batch = ox_batch.permute(1,0,2).reshape(batch_size*max_seq_len, -1)\n",
        "                y_batch = y_batch.reshape(-1)\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                loss = self.criterion(ox_batch, y_batch)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            end_time = time.time() - start_time\n",
        "            end_time = str(datetime.timedelta(seconds=end_time))[:-7]\n",
        "            print('Time [%s], Epoch [%d/%d], loss: %.4f'\n",
        "                  % (end_time, epoch+1, epochs, np.mean(losses)))\n",
        "            if epoch % 5 == 0:\n",
        "                generated_sentences = self.test()\n",
        "                print('[Generated Sentences]')\n",
        "                for sentence in generated_sentences:\n",
        "                    print(sentence)\n",
        "            \n",
        "    def test(self):\n",
        "        # Test model to genereate the sentences\n",
        "        self.model.eval()\n",
        "        num_sentence = 5\n",
        "        max_seq_len = 50\n",
        "\n",
        "        # initial hidden-states\n",
        "        outs = []\n",
        "        x = torch.randint(0, 10000, (num_sentence,)).to(self.device)\n",
        "        hx = torch.zeros(num_sentence, hidden_size).to(self.device)\n",
        "        cx = torch.zeros(num_sentence, hidden_size).to(self.device)\n",
        "\n",
        "        outs.append(x)\n",
        "        with torch.no_grad():\n",
        "            for s_idx in range(max_seq_len-1):\n",
        "                x, hx, cx = self.model(x, hx, cx, predict=True)\n",
        "                outs.append(x)\n",
        "        outs = torch.cat(outs).reshape(max_seq_len, num_sentence)\n",
        "        outs = outs.permute(1, 0)\n",
        "        outs = outs.detach().cpu().numpy()\n",
        "\n",
        "        sentences = []\n",
        "        for out in outs:\n",
        "            sentence = []\n",
        "            for token_idx in out:\n",
        "                word = self.idx2word[token_idx]\n",
        "                sentence.append(word)\n",
        "            sentences.append(sentence)\n",
        "       \n",
        "        return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgEJv1vWqNkS"
      },
      "source": [
        "lr = 1e-2\n",
        "input_size = 128\n",
        "hidden_size = 128\n",
        "batch_size = 256\n",
        "\n",
        "dataset = LMDataset(tokens)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "model = LanguageModel(input_size=input_size, hidden_size=hidden_size)\n",
        "# NOTE: you should use ignore_index to ignore the loss from predicting the <PAD> token\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "device = torch.device('cuda')\n",
        "\n",
        "trainer = Trainer(word2idx = word2idx,\n",
        "                  idx2word = idx2word,\n",
        "                  dataloader=dataloader, \n",
        "                  model = model,\n",
        "                  criterion=criterion,\n",
        "                  optimizer = optimizer,\n",
        "                  device=device)\n",
        "\n",
        "trainer.train(epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDhlrcENM4Dx"
      },
      "source": [
        "생성된 텍스트의 퀄리티는 어떤가요? \n",
        "\n",
        "앞으로 딥러닝 강의가 끝나면 자연어처리 강좌에서 본격텍스트 처리에 적합한 전처리, 모델구조, 학습 trick들을 배우시게 될것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ua-_6W2a5Lt"
      },
      "source": [
        "# References\n",
        "\n",
        "1. https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "2. https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model"
      ]
    }
  ]
}