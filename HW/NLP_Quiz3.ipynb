{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Quiz3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGdFkr3AJtsB"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEUJ5AVJp9K"
      },
      "source": [
        "다음 주관식 질문에 올바른 답을 작성하시오."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dkp5OZfJulS"
      },
      "source": [
        "1. bert-base-uncased의 tokenizer를 이용해 \"I love natural language processing\"을 tokenize한 뒤 token, id, string을 반환하는 코드를 transformers 라이브러리를 이용해 작성하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsXfvF0DLX7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5WTkwS4NTT8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ooI72AZPArI"
      },
      "source": [
        "2. transformers의 pipeline class를 이용해 bert-base-uncased 모델을 load하고 \"I [MASK] natural language processing\"에서 \"[MASK]\"에 대한 예측값 상위 5개를 출력하는 코드를 작성하세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTxtdeEvMfmT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7iUSURdLmco"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJH8r7exVqIY"
      },
      "source": [
        "3. bert-base-uncased의 tokenizer에서 1002번째 token의 string을 출력하는 코드를 작성하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-1yCvmOVpqM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ16hCfve--v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLy3bYMFPzlI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chzlbUsUWvfw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAYBe3kkmXrz"
      },
      "source": [
        "4. 다음 출력 문장과 정답 문장을 이용해 BLEU score를 계산하시오. (BLEU score 계산법은 강의자료를 따라주세요.) \n",
        "\n",
        "Brevity Score: 0.5  \n",
        "Predicted Sentence: I like nlp but not you  \n",
        "Reference: I love natural language processing how about you ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyHBp2luIurU"
      },
      "source": [
        "다음 명제에 대해, True/False를 판단하시오. 판단 근거를 간략하게 설명하시오."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqX6eEzXIzqJ"
      },
      "source": [
        "1. GPT-1,2,3 은 Bi-directional Transformer이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7EyLImgj18s"
      },
      "source": [
        "2. GPT-1은 여러가지의 task를 하나의 모델로 수행할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Y_2sSlkRUU"
      },
      "source": [
        "3. BERT는 classification을 하기 위해 encoder(transformer block)의 output을 모두 활용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10bqA1ivlBme"
      },
      "source": [
        "4. BERT의 [CLS] token은 반드시 첫번째 sequence에 위치 해야한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj0wFqQjl8TS"
      },
      "source": [
        "5. 두 문장의 의미가 서로 같은지 다른지 판단하는 task를 BERT를 이용하여 수행할 때 문장의 위치가 서로 바뀌더라도 성능은 동일하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BdOO525d_Tu"
      },
      "source": [
        "### build_bpe 함수를 완성해주세요.\n",
        "\n",
        "byte pair encoding을 이용한 간단한 sub-word tokenizer를 구현해봅니다.\n",
        "과제 노트북의 지시사항과 각 함수의 docstring과 [논문](https://arxiv.org/pdf/1508.07909.pdf)의 3페이지 algorithm 1 참고하여 build_bpe 함수를 완성하고 모든 test case를 통과해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGOaLtUQd4D1"
      },
      "source": [
        "from typing import List, Dict, Set\n",
        "from itertools import chain\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "def build_bpe(\n",
        "        corpus: List[str],\n",
        "        max_vocab_size: int\n",
        ") -> List[int]:\n",
        "    \"\"\" BPE Vocabulary Builder\n",
        "    Implement vocabulary builder for byte pair encoding.\n",
        "    Please sort your idx2word by subword length in descending manner.\n",
        "\n",
        "    Hint: Counter in collection library would be helpful\n",
        "\n",
        "    Note: If you convert sentences list to word frequence dictionary,\n",
        "          building speed is enhanced significantly because duplicated words are\n",
        "          preprocessed together\n",
        "\n",
        "    Arguments:\n",
        "    corpus -- List of words to build vocab\n",
        "    max_vocab_size -- The maximum size of vocab\n",
        "\n",
        "    Return:\n",
        "    idx2word -- Subword list\n",
        "    \"\"\"\n",
        "    # Special tokens\n",
        "    PAD = BytePairEncoding.PAD_token  # Index of <PAD> must be 0\n",
        "    UNK = BytePairEncoding.UNK_token  # Index of <UNK> must be 1\n",
        "    CLS = BytePairEncoding.CLS_token  # Index of <CLS> must be 2\n",
        "    SEP = BytePairEncoding.SEP_token  # Index of <SEP> must be 3\n",
        "    MSK = BytePairEncoding.MSK_token  # Index of <MSK> must be 4\n",
        "    SPECIAL = [PAD, UNK, CLS, SEP, MSK]\n",
        "\n",
        "    WORD_END = BytePairEncoding.WORD_END  # Use this token as the end of a word\n",
        "    # YOUR CODE HERE\n",
        "    id2word = None\n",
        "    \n",
        "    return idx2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnu8nACHkLuL"
      },
      "source": [
        "#############################################\n",
        "# Helper functions below. DO NOT MODIFY!    #\n",
        "#############################################\n",
        "\n",
        "class BytePairEncoding(object):\n",
        "    \"\"\" Byte Pair Encoding class\n",
        "    We aren't gonna use this class for encoding. Because it is too slow......\n",
        "    We will use sentence piece Google have made.\n",
        "    Thus, this class is just for special token index reference.\n",
        "    \"\"\"\n",
        "    PAD_token = '<pad>'\n",
        "    PAD_token_idx = 0\n",
        "    UNK_token = '<unk>'\n",
        "    UNK_token_idx = 1\n",
        "    CLS_token = '<cls>'\n",
        "    CLS_token_idx = 2\n",
        "    SEP_token = '<sep>'\n",
        "    SEP_token_idx = 3\n",
        "    MSK_token = '<msk>'\n",
        "    MSK_token_idx = 4\n",
        "\n",
        "    WORD_END = '_'\n",
        "\n",
        "    def __init__(self, corpus: List[List[str]], max_vocab_size: int) -> None:\n",
        "        self.idx2word = build_bpe(corpus, max_vocab_size)\n",
        "\n",
        "    def encode(self, sentence: List[str]) -> List[int]:\n",
        "        return encode(sentence, self.idx2word)\n",
        "\n",
        "    def decoder(self, tokens: List[int]) -> List[str]:\n",
        "        return decode(tokens, self.idx2word)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Testing functions below.                  #\n",
        "#############################################\n",
        "\n",
        "\n",
        "def test_build_bpe():\n",
        "    print(\"======Building BPE Vocab Test Case======\")\n",
        "    PAD = BytePairEncoding.PAD_token\n",
        "    UNK = BytePairEncoding.UNK_token\n",
        "    CLS = BytePairEncoding.CLS_token\n",
        "    SEP = BytePairEncoding.SEP_token\n",
        "    MSK = BytePairEncoding.MSK_token\n",
        "    WORD_END = BytePairEncoding.WORD_END\n",
        "\n",
        "    # First test\n",
        "    corpus = ['abcde']\n",
        "    vocab = build_bpe(corpus, max_vocab_size=15)\n",
        "    assert vocab[:5] == [PAD, UNK, CLS, SEP, MSK], \\\n",
        "        \"Please insert the special tokens properly\"\n",
        "    print(\"The first test passed!\")\n",
        "\n",
        "    # Second test\n",
        "    assert sorted(vocab[5:], key=len, reverse=True) == vocab[5:], \\\n",
        "        \"Please sort your idx2word by subword length in decsending manner.\"\n",
        "    print(\"The second test passed!\")\n",
        "\n",
        "    # Third test\n",
        "    corpus = ['low'] * 5 + ['lower'] * 2 + ['newest'] * 6 + ['widest'] * 3\n",
        "    vocab = set(build_bpe(corpus, max_vocab_size=24))\n",
        "    assert vocab > {PAD, UNK, CLS, SEP, MSK, 'est_', 'low', 'newest_', \\\n",
        "                    'i', 'e', 'n', 't', 'd', 's', 'o', 'l', 'r', 'w',\n",
        "                    WORD_END} and \\\n",
        "           \"low_\" not in vocab and \"wi\" not in vocab and \"id\" not in vocab, \\\n",
        "        \"Your bpe result does not match expected result\"\n",
        "    print(\"The third test passed!\")\n",
        "\n",
        "    # forth test\n",
        "    corpus = ['aaaaaaaaaaaa', 'abababab']\n",
        "    vocab = set(build_bpe(corpus, max_vocab_size=13))\n",
        "    assert vocab == {PAD, UNK, CLS, SEP, MSK, 'aaaaaaaa', 'aaaa', 'abab', 'aa',\n",
        "                     'ab', 'a', 'b', WORD_END}, \\\n",
        "        \"Your bpe result does not match expected result\"\n",
        "    print(\"The forth test passed!\")\n",
        "\n",
        "    # fifth test\n",
        "    corpus = ['abc', 'bcd']\n",
        "    vocab = build_bpe(corpus, max_vocab_size=10000)\n",
        "    assert len(vocab) == 15, \\\n",
        "        \"Your bpe result does not match expected result\"\n",
        "    print(\"The fifth test passed!\")\n",
        "\n",
        "    print(\"All 5 tests passed!\")\n",
        "test_build_bpe()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}