{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "[HW25_Problem] Topic Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUEqzkNmnyYH"
      },
      "source": [
        "# [HW2] Topic Modeling\n",
        "1. Crawling News\n",
        "2. Preprocessing\n",
        "3. Build Term-Document Matrix\n",
        "4. Topic modeling\n",
        "5. Visualization\n",
        "\n",
        "```\n",
        "🔥 이번 시간에는 Topic Modeling를 직접 크롤링한 뉴스 데이터에 대해서 수행해보는 시간을 갖겠습니다. \n",
        "\n",
        "먼저 네이버에서 뉴스 기사를 간단하게 크롤링합니다.\n",
        "기본적인 전처리 이후 Term-document Matrix를 만들고 이를 non-negative factorization을 이용해 행렬 분해를 하여 Topic modeling을 수행합니다.\n",
        "\n",
        "t-distributed stochastic neighbor embedding(T-SNE) 기법을 이용해 Topic별 시각화를 진행합니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIARcrg_oNMN"
      },
      "source": [
        "!pip install newspaper3k\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPu9TCFEoK_m"
      },
      "source": [
        "# 크롤링에 필요한 패키지 설치\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "from time import sleep\n",
        "from time import time\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import datetime\n",
        "from multiprocessing import Pool\n",
        "import json\n",
        "import requests\n",
        "import re\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU2ipk6ZvZXf"
      },
      "source": [
        "```\n",
        "💡 Crawling(크롤링)이란?\n",
        "\n",
        "크롤링은 웹 페이지에서 필요한 데이터를 추출해내는 작업을 말합니다.\n",
        "이번 시간에는 정적 페이지인 네이버의 뉴스 신문 기사 웹페이지를 크롤링합니다.\n",
        "\n",
        "HTML은 설명되어 있는 자료가 많기 때문에 생략하도록 하겠습니다.\n",
        "HTML 구조 파악 및 태그에 대한 설명은 아래 참고자료를 살펴봐주세요 !\n",
        "```\n",
        "\n",
        "참고: [위키피디아: 정적페이지](https://ko.wikipedia.org/wiki/%EC%A0%95%EC%A0%81_%EC%9B%B9_%ED%8E%98%EC%9D%B4%EC%A7%80)\n",
        "\n",
        "참고: [생활코딩: HTML](https://opentutorials.org/course/2039)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0UYQWmsnHTF"
      },
      "source": [
        "def crawl_news(query: str=None, crawl_num: int=1000, workers: int=4):\n",
        "    '''뉴스 기사 텍스트가 담긴 list를 반환합니다.\n",
        "\n",
        "    Keyword arguments:\n",
        "    query -- 검색어 (default None)\n",
        "    crawl_num -- 수집할 뉴스 기사의 개수 (defualt 1000)\n",
        "    workers -- multi-processing시 사용할 thread의 개수 (default 4)\n",
        "    '''\n",
        "\n",
        "    url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'\n",
        "    articleList = []\n",
        "    crawled_url = set()\n",
        "    keyboard_interrupt = False\n",
        "    t = time()\n",
        "    idx = 0\n",
        "    page = 1\n",
        "\n",
        "    \n",
        "    # 서버에 url 요청의 결과를 선언\n",
        "    res = requests.get(url.format(query))\n",
        "    sleep(0.5)\n",
        "    # res를 parsing할 parser를 선언\n",
        "    bs = BeautifulSoup(res.text, 'html.parser')\n",
        "    \n",
        "    with Pool(workers) as p:\n",
        "        while idx < crawl_num:            \n",
        "            table = bs.find('ul', {'class': 'list_news'})\n",
        "            li_list = table.find_all('li', {'id': re.compile('sp_nws.*')})\n",
        "            area_list = [li.find('div', {'class':'news_area'}) for li in li_list]\n",
        "            a_list = [area.find('a', {'class':'news_tit'}) for area in area_list]\n",
        "            \n",
        "            for n in a_list[:min(len(a_list), crawl_num-idx)]:\n",
        "                articleList.append(n.get('title'))\n",
        "                idx += 1\n",
        "            page += 1\n",
        "\n",
        "            pages = bs.find('div', {'class': 'sc_page_inner'})\n",
        "            next_page_url = [p for p in pages.find_all('a') if p.text == str(page)][0].get('href')\n",
        "\n",
        "            req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n",
        "            bs = BeautifulSoup(req.text, 'html.parser')\n",
        "    return articleList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ln1Wih0XVnt"
      },
      "source": [
        "```\n",
        "🔥 이제 '구글'이라는 이름으로 뉴스 기사 1000개의 제목을 크롤링하겠습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6CAFa6J56yJ"
      },
      "source": [
        "query = '구글'\n",
        "\n",
        "articleList = crawl_news(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDIS5V1yAcRT"
      },
      "source": [
        "articleList[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9HG4LxOaa1r"
      },
      "source": [
        "```\n",
        "🔥 태거(tagger)를 이용해 한글 명사와 알파벳만을 추출해서 term-document matrix (tdm)을 만들겠습니다.\n",
        "\n",
        "태거(tagger)는 tokenization에서 조금 더 자세히 다루도록 하겠습니다.\n",
        "```\n",
        "\n",
        "참고: [konlpy: morph analyzer](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-15T06:17:04.018304Z",
          "start_time": "2019-05-15T06:09:10.729214Z"
        },
        "id": "Mdr-aTytaCLr"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-29T08:48:53.790086Z",
          "start_time": "2019-05-29T08:46:38.984818Z"
        },
        "id": "HUN9k5DXaCLs"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# Okt 형태소 분석기 선언\n",
        "t = Okt()\n",
        "\n",
        "words_list_ = []\n",
        "vocab = Counter()\n",
        "tag_set = set(['Noun', 'Alpha'])\n",
        "stopwords = set(['글자'])\n",
        "\n",
        "for i, article in enumerate(articleList):\n",
        "    if i % 100 == 0:\n",
        "        print(i)\n",
        "    \n",
        "    # tagger를 이용한 품사 태깅\n",
        "    words = t.pos(article, norm=True, stem=True)\n",
        "\n",
        "    ############################ ANSWER HERE ################################\n",
        "    # TODO: 다음의 조건을 만족하는 단어의 리스트를 완성하세요.\n",
        "    # 조건 1: 명사와 알파벳 tag를 가진 단어\n",
        "    # 조건 2: 철자 길이가 2이상인 단어 \n",
        "    # 조건 3: stopwords에 포함되지 않는 단어\n",
        "    #########################################################################        \n",
        "\n",
        "    vocab.update(words)\n",
        "    words_list_.append((words, article))\n",
        "    \n",
        "vocab = sorted([w for w, freq in vocab.most_common(10000)])\n",
        "word2id = {w: i for i, w in enumerate(vocab)}\n",
        "words_list = []\n",
        "for words, article in words_list_:\n",
        "    words = [w for w in words if w in word2id]\n",
        "    if len(words) > 10:\n",
        "        words_list.append((words, article))\n",
        "        \n",
        "del words_list_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L1BkQeaaCLv"
      },
      "source": [
        "## Build document-term matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5YfokxJf86R"
      },
      "source": [
        "```\n",
        "🔥 이제 document-term matrix를 만들어보겠습니다.\n",
        "document-term matrix는 (문서 개수 x 단어 개수)의 Matrix입니다.\n",
        "```\n",
        "\n",
        "참고: [Document-Term Matrix](https://wikidocs.net/24559)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-29T08:48:56.005889Z",
          "start_time": "2019-05-29T08:48:53.792571Z"
        },
        "id": "MI5weWREaCLv"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import numpy as np\n",
        "\n",
        "dtm = np.zeros((len(words_list), len(vocab)), dtype=np.float32)\n",
        "for i, (words, article) in enumerate(words_list):\n",
        "    for word in words:\n",
        "        dtm[i, word2id[word]] += 1\n",
        "        \n",
        "dtm = TfidfTransformer().fit_transform(dtm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0xYqpVtmgoL"
      },
      "source": [
        "```\n",
        "🔥 document-term matrix를 non-negative factorization(NMF)을 이용해 행렬 분해를 해보겠습니다.\n",
        "\n",
        "💡 Non-negative Factorization이란?\n",
        "\n",
        "NMF는 주어진 행렬 non-negative matrix X를 non-negative matrix W와 H로 행렬 분해하는 알고리즘입니다.\n",
        "이어지는 코드를 통해 W와 H의 의미에 대해 파악해봅시다.\n",
        "```\n",
        "참고: [Non-negative Matrix Factorization](https://angeloyeo.github.io/2020/10/15/NMF.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRWNuIguaCLy"
      },
      "source": [
        "## Topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erhwS2ntQBSD"
      },
      "source": [
        "# Non-negative Matrix Factorization\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "K=5\n",
        "nmf = NMF(n_components=K, alpha=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I24Y78G3VvO"
      },
      "source": [
        "```\n",
        "🔥 sklearn의 NMF를 이용해 W와 H matrix를 구해봅시다.\n",
        "W는 document length x K, H는 K x term length의 차원을 갖고 있습니다.\n",
        "W의 하나의 row는 각각의 feature에 얼만큼의 가중치를 줄 지에 대한 weight입니다.\n",
        "H의 하나의 row는 하나의 feature를 나타냅니다.\n",
        "\n",
        "우선 하나의 Topic (H의 n번째 row)에 접근해서 해당 topic에 대해 값이 가장 높은 20개의 단어를 출력해보겠습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2TY6gu4QH1o"
      },
      "source": [
        "W = nmf.fit_transform(dtm)\n",
        "H = nmf.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbNWlTAv5Zn2"
      },
      "source": [
        "```\n",
        "🔥 우선 하나의 Topic (H의 n번째 row)에 접근해서 해당 topic에 대해 값이 가장 높은 20개의 단어를 출력해보겠습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-29T08:48:58.523062Z",
          "start_time": "2019-05-29T08:48:58.500171Z"
        },
        "id": "uSzB5PBuaCL2"
      },
      "source": [
        "for k in range(K):\n",
        "    print(f\"{k}th topic\")\n",
        "    for index in H[k].argsort()[::-1][:20]:\n",
        "        print(vocab[index], end=' ')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTP6lUft5C4j"
      },
      "source": [
        "```\n",
        "🔥 이번에는 W에서 하나의 Topic (W의 n번째 column)에 접근해서 해당 topic에 대해 값이 가장 높은 3개의 뉴스 기사 제목을 출력해보겠습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-29T08:48:58.553639Z",
          "start_time": "2019-05-29T08:48:58.525169Z"
        },
        "scrolled": false,
        "id": "K1CniIn7aCL5"
      },
      "source": [
        "for k in range(K):\n",
        "    print(f\"==={k}th topic===\")\n",
        "    for index in W[:, k].argsort()[::-1][:3]:\n",
        "        print(words_list[index][1])\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLOplJlO5tFi"
      },
      "source": [
        "```\n",
        "❓ 2번째 토픽에 대해 가장 높은 가중치를 갖는 제목 5개를 출력해볼까요?\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-29T08:48:58.574010Z",
          "start_time": "2019-05-29T08:48:58.567122Z"
        },
        "id": "Jfm4fK4jaCL9"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7AZ3SyS6Y5L"
      },
      "source": [
        "```\n",
        "🔥 이번에는 t-SNE를 이용해 Topic별 시각화를 진행해보겠습니다.\n",
        "\n",
        "💡 t-SNE는 무엇인가요?\n",
        "\n",
        "t-Stochastic Neighbor Embedding(t-SNE)은 고차원의 벡터를 \n",
        "저차원(2~3차원) 벡터로 데이터간 구조적 특징을 유지하며 축소를 하는 방법 중 하나입니다.\n",
        "\n",
        "주로 고차원 데이터의 시각화를 위해 사용됩니다.\n",
        "```\n",
        "\n",
        "참고: [lovit: t-SNE](https://lovit.github.io/nlp/representation/2018/09/28/tsne/#:~:text=t%2DSNE%20%EB%8A%94%20%EA%B3%A0%EC%B0%A8%EC%9B%90%EC%9D%98,%EC%9D%98%20%EC%A7%80%EB%8F%84%EB%A1%9C%20%ED%91%9C%ED%98%84%ED%95%A9%EB%8B%88%EB%8B%A4.)\n",
        "\n",
        "참고: [ratsgo: t-SNE](https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1HlyoakaCMA"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-29T08:49:22.529080Z",
          "start_time": "2019-05-29T08:48:58.612549Z"
        },
        "id": "rqajA3lDaCMB"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# n_components = 차원 수\n",
        "tsne = TSNE(n_components=2, init='pca', verbose=1)\n",
        "\n",
        "# W matrix에 대해 t-sne를 수행합니다.\n",
        "W2d = tsne.fit_transform(W)\n",
        "\n",
        "# 각 뉴스 기사 제목마다 가중치가 가장 높은 topic을 저장합니다.\n",
        "topicIndex = [v.argmax() for v in W]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-16T08:24:10.840813Z",
          "start_time": "2019-05-16T08:24:10.695706Z"
        },
        "scrolled": true,
        "id": "pc76X-jSaCME"
      },
      "source": [
        "from bokeh.models import HoverTool\n",
        "from bokeh.palettes import Category20\n",
        "from bokeh.io import show, output_notebook\n",
        "from bokeh.plotting import figure, ColumnDataSource\n",
        "output_notebook()\n",
        "\n",
        "# 사용할 툴들\n",
        "tools_to_show = 'hover,box_zoom,pan,save,reset,wheel_zoom'\n",
        "p = figure(plot_width=720, plot_height=580, tools=tools_to_show)\n",
        "\n",
        "source = ColumnDataSource(data={\n",
        "    'x': W2d[:, 0],\n",
        "    'y': W2d[:, 1],\n",
        "    'id': [i for i in range(W.shape[0])],\n",
        "    'document': [article for words, article in words_list],\n",
        "    'topic': [str(i) for i in topicIndex],  # 토픽 번호\n",
        "    'color': [Category20[K][i] for i in topicIndex]\n",
        "})\n",
        "p.circle(\n",
        "    'x', 'y',\n",
        "    source=source,\n",
        "    legend='topic',\n",
        "    color='color'\n",
        ")\n",
        "\n",
        "# interaction\n",
        "p.legend.location = \"top_left\"\n",
        "hover = p.select({'type': HoverTool})\n",
        "hover.tooltips = [(\"Topic\", \"@topic\"), ('id', '@id'), (\"Article\", \"@document\")]\n",
        "hover.mode = 'mouse'\n",
        "\n",
        "show(p)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}